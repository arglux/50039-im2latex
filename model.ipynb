{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# imports and global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import csv\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available: True\n"
     ]
    }
   ],
   "source": [
    "# Define device for torch\n",
    "use_cuda = True\n",
    "print(\"CUDA is available:\", torch.cuda.is_available())\n",
    "device = torch.device(\"cuda\" if (use_cuda and torch.cuda.is_available()) else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths\n",
    "formulas_ref = \"im2latex_formulas.lst\"\n",
    "training_ref = \"im2latex_train.lst\"\n",
    "validation_ref = \"im2latex_validate.lst\"\n",
    "test_ref = \"im2latex_test.lst\"\n",
    "img_path = \"formula_images\"\n",
    "trainingcsv = \"training.csv\"\n",
    "validationcsv = \"validation.csv\"\n",
    "testcsv = \"test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "DEBUG = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# udpate all label files with their formulas (already ran, don't rerun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "everything = []\n",
    "\n",
    "def takezero(elem):\n",
    "    return int(elem[0])\n",
    "\n",
    "with open(training_ref, 'r') as training_label_file:\n",
    "    while True:\n",
    "        line = training_label_file.readline().rstrip(\"\\n\").split()\n",
    "        if len(line) < 2:\n",
    "            break\n",
    "        line.append('training')\n",
    "        everything.append(line)\n",
    "        \n",
    "with open(validation_ref, 'r') as validation_label_file:\n",
    "    while True:\n",
    "        line = validation_label_file.readline().rstrip(\"\\n\").split()\n",
    "        if len(line) < 2:\n",
    "            break\n",
    "        line.append('validation')\n",
    "        everything.append(line)\n",
    "        \n",
    "with open(test_ref, 'r') as test_label_file:\n",
    "    while True:\n",
    "        line = test_label_file.readline().rstrip(\"\\n\").split()\n",
    "        if len(line) < 2:\n",
    "            break\n",
    "        line.append('test')\n",
    "        everything.append(line)\n",
    "        \n",
    "# sort the list out\n",
    "everything.sort(key=takezero)\n",
    "\n",
    "with open(formulas_ref, 'r', newline=\"\\n\", encoding=\"latin-1\") as formula_list:\n",
    "    for labels in everything:\n",
    "        formula = formula_list.readline().rstrip(\"\\n\")\n",
    "        labels.append(formula)\n",
    "        \n",
    "# split back into the new files\n",
    "with open(trainingcsv, \"w\") as trainingfile:\n",
    "    writer = csv.writer(trainingfile, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    for i in everything:\n",
    "        if i[3] == \"training\":\n",
    "            writer.writerow(i)\n",
    "\n",
    "with open(validationcsv, \"w\") as validationfile:\n",
    "    writer = csv.writer(validationfile, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    for i in everything:\n",
    "        if i[3] == \"validation\":\n",
    "            writer.writerow(i)\n",
    "            \n",
    "with open(testcsv, \"w\") as testfile:\n",
    "    writer = csv.writer(testfile, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    for i in everything:\n",
    "        if i[3] == \"test\":\n",
    "            writer.writerow(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fetch variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '60ee748793', 'basic', 'training', 'ds^{2} = (1 - {qcos\\\\theta\\\\over r})^{2\\\\over 1 + \\\\alpha^{2}}\\\\lbrace dr^2+r^2d\\\\theta^2+r^2sin^2\\\\theta d\\\\varphi^2\\\\rbrace -{dt^2\\\\over  (1 - {qcos\\\\theta\\\\over r})^{2\\\\over 1 + \\\\alpha^{2}}}\\\\, .\\\\label{eq:sps1}']\n",
      "['0', '5abbb9b19f', 'basic', 'validation', \"\\\\int_{-\\\\epsilon}^\\\\infty dl\\\\: {\\\\rm e}^{-l\\\\zeta}\\t\\\\int_{-\\\\epsilon}^\\\\infty dl' {\\\\rm e}^{-l'\\\\zeta}\\tll'{l'-l \\\\over l+l'} \\\\{3\\\\,\\\\delta''(l) - {3 \\\\over 4}t\\\\,\\\\delta(l) \\\\} =0.\\t\\t\\\\label{eq21}\"]\n",
      "['11', '15b9034ba8', 'basic', 'test', '\\\\label{fierep}P_{(2)}^-=\\\\int \\\\beta d\\\\beta d^9p d^8\\\\lambda \\\\Phi(-p,-\\\\lambda)\\\\left(-\\\\frac{p^Ip^I}{2\\\\beta}\\\\right) \\\\Phi(p,\\\\lambda)\\\\,.']\n"
     ]
    }
   ],
   "source": [
    "# index, image_name, rendering_type, type, formula\n",
    "training_formulas = []\n",
    "\n",
    "with open(trainingcsv, 'r', encoding=\"latin-1\") as training_csv:\n",
    "    reader = csv.reader(training_csv, delimiter=',')\n",
    "    for row in reader:\n",
    "        training_formulas.append(row)\n",
    "        \n",
    "        \n",
    "# index, image_name, rendering_type, type, formula\n",
    "validation_formulas = []\n",
    "\n",
    "with open(validationcsv, 'r', encoding=\"latin-1\") as validation_csv:\n",
    "    reader = csv.reader(validation_csv, delimiter=',')\n",
    "    for row in reader:\n",
    "        validation_formulas.append(row)\n",
    "        \n",
    "        \n",
    "# index, image_name, rendering_type, type, formula\n",
    "test_formulas = []\n",
    "\n",
    "with open(testcsv, 'r', encoding=\"latin-1\") as test_csv:\n",
    "    reader = csv.reader(test_csv, delimiter=',')\n",
    "    for row in reader:\n",
    "        test_formulas.append(row)\n",
    "        \n",
    "for i in training_formulas:\n",
    "    print(i)\n",
    "    break\n",
    "    \n",
    "for i in validation_formulas:\n",
    "    print(i)\n",
    "    break\n",
    "    \n",
    "for i in test_formulas:\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Latex_Dataset(Dataset):\n",
    "    def __init__(self, input_arr, img_path):\n",
    "        self.latex_arr = input_arr \n",
    "        self.img_path = img_path\n",
    "        \n",
    "    def describe(self):\n",
    "#         print(\"image sizes of {} by {}\".format(self.img_size[0], self.img_size[1]))\n",
    "        print(\"length of training: {}\".format(len(self.latex_arr)))\n",
    "    \n",
    "    def open_img(self, index):\n",
    "        \"\"\"\n",
    "        arr_type: train / validation / test\n",
    "        index: index from the respective array\n",
    "        \"\"\"        \n",
    "        err_msg = \"index exceeds array length\"\n",
    "        assert len(self.latex_arr) > index, err_msg\n",
    "        \n",
    "        # open the file\n",
    "        path_to_file = self.img_path + '/' + self.latex_arr[index][1] + \".png\"\n",
    "        with open(path_to_file, \"rb\") as f:\n",
    "            im = np.asarray(Image.open(f))/255\n",
    "        f.close()\n",
    "        return im\n",
    "    \n",
    "    def show_img(self, index):\n",
    "        im = self.open_img(index)\n",
    "        plt.imshow(im)\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        arr_type: train / validation / test\n",
    "        \"\"\"\n",
    "        return len(self.latex_arr)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        arr_type: train / validation / test\n",
    "        index: index from the respective array\n",
    "        \n",
    "        returns the entire entry at index\n",
    "        \"\"\"\n",
    "        \n",
    "        err_msg = \"index exceeds array length\"\n",
    "        assert len(self.latex_arr) > index, err_msg\n",
    "        \n",
    "        item = self.latex_arr[index]\n",
    "        if DEBUG:\n",
    "            print(\"overall index:\", item[0])\n",
    "            print(\"index in array:\", index)\n",
    "            print(\"image_name: {}\".format(item[1]+\".png\"))\n",
    "            print(\"rendering_type:\", item[2])\n",
    "            print(\"latex formula:\", item[4])\n",
    "\n",
    "        # return image, formula, rendering_type\n",
    "        image = self.open_img(index)\n",
    "        image = transforms.functional.to_tensor(np.array(image)).float()\n",
    "        return image, item[4], item[2]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]]), 'ds^{2} = (1 - {qcos\\\\theta\\\\over r})^{2\\\\over 1 + \\\\alpha^{2}}\\\\lbrace dr^2+r^2d\\\\theta^2+r^2sin^2\\\\theta d\\\\varphi^2\\\\rbrace -{dt^2\\\\over  (1 - {qcos\\\\theta\\\\over r})^{2\\\\over 1 + \\\\alpha^{2}}}\\\\, .\\\\label{eq:sps1}', 'basic']\n",
      "[tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]]), \"\\\\int_{-\\\\epsilon}^\\\\infty dl\\\\: {\\\\rm e}^{-l\\\\zeta}\\t\\\\int_{-\\\\epsilon}^\\\\infty dl' {\\\\rm e}^{-l'\\\\zeta}\\tll'{l'-l \\\\over l+l'} \\\\{3\\\\,\\\\delta''(l) - {3 \\\\over 4}t\\\\,\\\\delta(l) \\\\} =0.\\t\\t\\\\label{eq21}\", 'basic']\n",
      "[tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]]), '\\\\label{fierep}P_{(2)}^-=\\\\int \\\\beta d\\\\beta d^9p d^8\\\\lambda \\\\Phi(-p,-\\\\lambda)\\\\left(-\\\\frac{p^Ip^I}{2\\\\beta}\\\\right) \\\\Phi(p,\\\\lambda)\\\\,.', 'basic']\n"
     ]
    }
   ],
   "source": [
    "# jes: size not fixed yet idk what to do with it\n",
    "ld_train = Latex_Dataset(training_formulas, img_path)\n",
    "train_loader = DataLoader(ld_train, batch_size=BATCH_SIZE)\n",
    "for batch in train_loader:\n",
    "    print([item[0] for item in batch])\n",
    "    break\n",
    "\n",
    "ld_validation = Latex_Dataset(validation_formulas, img_path)\n",
    "validation_loader = DataLoader(ld_validation, batch_size=BATCH_SIZE)\n",
    "for batch in validation_loader:\n",
    "    print([item[0] for item in batch])\n",
    "    break\n",
    "\n",
    "ld_test = Latex_Dataset(test_formulas, img_path)\n",
    "test_loader = DataLoader(ld_test, batch_size=BATCH_SIZE)\n",
    "\n",
    "for batch in test_loader:\n",
    "    print([item[0] for item in batch])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# image processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_limits(arr):\n",
    "    \"\"\"\n",
    "    arr: boolean array.\n",
    "    Returns the left and rightmost indices which are True.\n",
    "    \"\"\"\n",
    "    return arr.argmax(), len(arr) - np.flip(arr).argmax() - 1\n",
    "\n",
    "def get_bounds(img):\n",
    "    \"\"\"\n",
    "    img: grayscale image with white background.\n",
    "    Returns the bounding box of the non-white area in the image.\n",
    "    \"\"\"\n",
    "    binarize = img != 255\n",
    "    vert = np.sum(binarize, axis=0) != 0\n",
    "    hori = np.sum(binarize, axis=1) != 0\n",
    "    vl, vr = find_limits(vert)\n",
    "    hl, hr = find_limits(hori)\n",
    "    return (vl, hl), (vr, hr)\n",
    "\n",
    "def center(img, vpad=None, hpad=None):\n",
    "    \"\"\"\n",
    "    img: grayscale image with white background.\n",
    "    Returns the image centered around the non-white region.\n",
    "    \"\"\"\n",
    "    (vl, hl), (vr, hr) = get_bounds(img)\n",
    "    roi = img[hl:hr+1, vl:vr+1]\n",
    "    vpad = (img.shape[0] - roi.shape[0]) if not vpad else vpad\n",
    "    hpad = (img.shape[1] - roi.shape[1]) if not hpad else hpad\n",
    "    return cv2.copyMakeBorder(roi, vpad//2, (vpad+1)//2, hpad//2, (hpad+1)//2, cv2.BORDER_CONSTANT, value=255)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import cm\n",
    "import PIL.ImageOps\n",
    "# get_bounds(train_loader.dataset.open_img(1))\n",
    "# find_limits(train_loader.dataset.open_img(1))\n",
    "im=train_loader.dataset.open_img(1)\n",
    "# max([max(x) for x in im])\n",
    "im = Image.fromarray(np.uint8(im*999999))\n",
    "inv = PIL.ImageOps.invert(im)\n",
    "# max(im[60])\n",
    "inv.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Vocab File in  vocab.pkl\n"
     ]
    }
   ],
   "source": [
    "from os.path import join\n",
    "import pickle as pkl\n",
    "from collections import Counter\n",
    "\n",
    "START_TOKEN = 0\n",
    "PAD_TOKEN = 1\n",
    "END_TOKEN = 2\n",
    "UNK_TOKEN = 3\n",
    "\n",
    "\n",
    "\n",
    "class Vocab(object):\n",
    "    def __init__(self):\n",
    "        self.sign2id = {\"<s>\": START_TOKEN, \"</s>\": END_TOKEN,\n",
    "                        \"<pad>\": PAD_TOKEN, \"<unk>\": UNK_TOKEN}\n",
    "        self.id2sign = dict((idx, token)\n",
    "                            for token, idx in self.sign2id.items())\n",
    "        self.length = 4\n",
    "\n",
    "    def add_sign(self, sign):\n",
    "        if sign not in self.sign2id:\n",
    "            self.sign2id[sign] = self.length\n",
    "            self.id2sign[self.length] = sign\n",
    "            self.length += 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "def build_vocab(min_count=10):\n",
    "    \"\"\"\n",
    "    traverse training formulas to make vocab\n",
    "    and store the vocab in the file\n",
    "    \"\"\"\n",
    "    vocab = Vocab()\n",
    "    counter = Counter()\n",
    "\n",
    "    with open('im2latex_formulas.norm.lst', 'r') as f:\n",
    "        formulas = [formula.strip('\\n') for formula in f.readlines()]\n",
    "\n",
    "    with open('im2latex_train_filter.lst', 'r') as f:\n",
    "        for line in f:\n",
    "            _, idx = line.strip('\\n').split()\n",
    "            idx = int(idx)\n",
    "            formula = formulas[idx].split()\n",
    "            counter.update(formula)\n",
    "\n",
    "    for word, count in counter.most_common():\n",
    "        if count >= min_count:\n",
    "            vocab.add_sign(word)\n",
    "    \n",
    "    vocab_file = 'vocab.pkl'\n",
    "    print(\"Writing Vocab File in \", vocab_file)\n",
    "    with open(vocab_file, 'wb') as w:\n",
    "        pkl.dump(vocab, w)\n",
    "    return vocab\n",
    "\n",
    "\n",
    "def load_vocab():\n",
    "    with open(join('vocab.pkl'), 'rb') as f:\n",
    "        vocab = pkl.load(f)\n",
    "    print(\"Load vocab including {} words!\".format(len(vocab)))\n",
    "    return vocab\n",
    "\n",
    "\n",
    "vocab = build_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '<s>',\n",
       " 2: '</s>',\n",
       " 1: '<pad>',\n",
       " 3: '<unk>',\n",
       " 4: '}',\n",
       " 5: '{',\n",
       " 6: '_',\n",
       " 7: '^',\n",
       " 8: '2',\n",
       " 9: '(',\n",
       " 10: ')',\n",
       " 11: '=',\n",
       " 12: '1',\n",
       " 13: '-',\n",
       " 14: ',',\n",
       " 15: '\\\\frac',\n",
       " 16: '+',\n",
       " 17: 'i',\n",
       " 18: '0',\n",
       " 19: 'x',\n",
       " 20: 'n',\n",
       " 21: '.',\n",
       " 22: '\\\\,',\n",
       " 23: 'd',\n",
       " 24: 'a',\n",
       " 25: '\\\\mu',\n",
       " 26: 'e',\n",
       " 27: 'k',\n",
       " 28: 'm',\n",
       " 29: 'r',\n",
       " 30: 'c',\n",
       " 31: 'p',\n",
       " 32: '\\\\partial',\n",
       " 33: '\\\\alpha',\n",
       " 34: 't',\n",
       " 35: 'A',\n",
       " 36: '~',\n",
       " 37: '\\\\;',\n",
       " 38: '3',\n",
       " 39: 'j',\n",
       " 40: 's',\n",
       " 41: 'l',\n",
       " 42: '\\\\left(',\n",
       " 43: '\\\\right)',\n",
       " 44: 'g',\n",
       " 45: '4',\n",
       " 46: '\\\\',\n",
       " 47: '\\\\nu',\n",
       " 48: '\\\\prime',\n",
       " 49: '\\\\pi',\n",
       " 50: 'z',\n",
       " 51: 'b',\n",
       " 52: '\\\\phi',\n",
       " 53: '|',\n",
       " 54: '\\\\mathrm',\n",
       " 55: '\\\\cal',\n",
       " 56: '\\\\delta',\n",
       " 57: 'f',\n",
       " 58: 'N',\n",
       " 59: 'q',\n",
       " 60: '\\\\lambda',\n",
       " 61: 'T',\n",
       " 62: 'S',\n",
       " 63: '\\\\beta',\n",
       " 64: ']',\n",
       " 65: 'R',\n",
       " 66: '[',\n",
       " 67: '\\\\bar',\n",
       " 68: '\\\\int',\n",
       " 69: 'D',\n",
       " 70: 'M',\n",
       " 71: 'L',\n",
       " 72: '\\\\operatorname',\n",
       " 73: 'B',\n",
       " 74: 'F',\n",
       " 75: '\\\\sigma',\n",
       " 76: 'y',\n",
       " 77: '&',\n",
       " 78: '\\\\\\\\',\n",
       " 79: '\\\\theta',\n",
       " 80: '\\\\gamma',\n",
       " 81: '\\\\psi',\n",
       " 82: 'h',\n",
       " 83: '/',\n",
       " 84: '\\\\hat',\n",
       " 85: '\\\\sqrt',\n",
       " 86: 'H',\n",
       " 87: '\\\\sum',\n",
       " 88: 'u',\n",
       " 89: '\\\\tilde',\n",
       " 90: '\\\\rho',\n",
       " 91: 'o',\n",
       " 92: '\\\\tau',\n",
       " 93: 'C',\n",
       " 94: 'G',\n",
       " 95: 'P',\n",
       " 96: 'V',\n",
       " 97: 'I',\n",
       " 98: 'E',\n",
       " 99: '\\\\omega',\n",
       " 100: 'X',\n",
       " 101: '\\\\epsilon',\n",
       " 102: 'J',\n",
       " 103: '\\\\bf',\n",
       " 104: '\\\\eta',\n",
       " 105: 'Q',\n",
       " 106: '\\\\Phi',\n",
       " 107: '\\\\xi',\n",
       " 108: 'v',\n",
       " 109: '\\\\quad',\n",
       " 110: '\\\\vec',\n",
       " 111: '\\\\Gamma',\n",
       " 112: 'K',\n",
       " 113: '\\\\infty',\n",
       " 114: '5',\n",
       " 115: '\\\\right]',\n",
       " 116: '\\\\left[',\n",
       " 117: 'U',\n",
       " 118: '\\\\Lambda',\n",
       " 119: '\\\\pm',\n",
       " 120: '\\\\dot',\n",
       " 121: 'W',\n",
       " 122: 'Z',\n",
       " 123: '\\\\begin{array}',\n",
       " 124: '\\\\end{array}',\n",
       " 125: '\\\\varphi',\n",
       " 126: '*',\n",
       " 127: '\\\\Delta',\n",
       " 128: '\\\\rangle',\n",
       " 129: '6',\n",
       " 130: 'w',\n",
       " 131: '\\\\chi',\n",
       " 132: '\\\\Omega',\n",
       " 133: ';',\n",
       " 134: '\\\\kappa',\n",
       " 135: '\\\\qquad',\n",
       " 136: '\\\\}',\n",
       " 137: '\\\\{',\n",
       " 138: '\\\\Psi',\n",
       " 139: '\\\\equiv',\n",
       " 140: '8',\n",
       " 141: '\\\\cdot',\n",
       " 142: '\\\\overline',\n",
       " 143: '\\\\!',\n",
       " 144: '\\\\langle',\n",
       " 145: '\\\\rightarrow',\n",
       " 146: '>',\n",
       " 147: '\\\\dagger',\n",
       " 148: '\\\\varepsilon',\n",
       " 149: '\\\\zeta',\n",
       " 150: '\\\\nabla',\n",
       " 151: '<',\n",
       " 152: 'O',\n",
       " 153: 'Y',\n",
       " 154: ':',\n",
       " 155: '\\\\Sigma',\n",
       " 156: '\\\\cdots',\n",
       " 157: '\\\\mathcal',\n",
       " 158: '\\\\ldots',\n",
       " 159: '\\\\ell',\n",
       " 160: '\\\\left\\\\{',\n",
       " 161: '\\\\:',\n",
       " 162: '\\\\sim',\n",
       " 163: '\\\\otimes',\n",
       " 164: '\\\\wedge',\n",
       " 165: '\\\\Pi',\n",
       " 166: '!',\n",
       " 167: '\\\\operatorname*',\n",
       " 168: '7',\n",
       " 169: '\\\\prod',\n",
       " 170: '\\\\hspace',\n",
       " 171: '\\\\hbar',\n",
       " 172: '\\\\in',\n",
       " 173: '\\\\vert',\n",
       " 174: '9',\n",
       " 175: '\\\\widetilde',\n",
       " 176: '\\\\right\\\\}',\n",
       " 177: '\\\\to',\n",
       " 178: '\\\\Big',\n",
       " 179: '\\\\Theta',\n",
       " 180: '\\\\mid',\n",
       " 181: '\\\\times',\n",
       " 182: '\\\\right|',\n",
       " 183: '\\\\mathbf',\n",
       " 184: '\\\\underline',\n",
       " 185: '\\\\ast',\n",
       " 186: '\\\\dots',\n",
       " 187: '\\\\leq',\n",
       " 188: '\\\\left|',\n",
       " 189: '\\\\approx',\n",
       " 190: '\\\\star',\n",
       " 191: '\\\\widehat',\n",
       " 192: '\\\\stackrel',\n",
       " 193: '\\\\right.',\n",
       " 194: '\\\\displaystyle',\n",
       " 195: '\\\\big',\n",
       " 196: '\\\\perp',\n",
       " 197: '\\\\left.',\n",
       " 198: '\\\\geq',\n",
       " 199: '\\\\mp',\n",
       " 200: '\\\\simeq',\n",
       " 201: '\\\\dag',\n",
       " 202: '\\\\vartheta',\n",
       " 203: '\\\\Bigr',\n",
       " 204: '\\\\right\\\\rangle',\n",
       " 205: \"'\",\n",
       " 206: '\\\\neq',\n",
       " 207: '\\\\Bigl',\n",
       " 208: '\\\\circ',\n",
       " 209: '\\\\longrightarrow',\n",
       " 210: '\\\\oint',\n",
       " 211: '\\\\biggl',\n",
       " 212: '\\\\biggr',\n",
       " 213: '\\\\bigg',\n",
       " 214: '\\\\textstyle',\n",
       " 215: '\\\\ddot',\n",
       " 216: '\\\\left\\\\langle',\n",
       " 217: '\\\\not',\n",
       " 218: '\\\\bigl',\n",
       " 219: '\\\\oplus',\n",
       " 220: '\\\\bigr',\n",
       " 221: '\\\\boldmath',\n",
       " 222: '\\\\Xi',\n",
       " 223: '\\\\propto',\n",
       " 224: '\\\\check',\n",
       " 225: '\\\\nonumber',\n",
       " 226: '\\\\triangle',\n",
       " 227: '\\\\le',\n",
       " 228: '\\\\varrho',\n",
       " 229: '\\\\ge',\n",
       " 230: '\\\\forall',\n",
       " 231: '\\\\scriptscriptstyle',\n",
       " 232: '\\\\imath',\n",
       " 233: '\\\\right>',\n",
       " 234: '\\\\|',\n",
       " 235: '--',\n",
       " 236: '\\\\lbrack',\n",
       " 237: '\\\\sp',\n",
       " 238: '\\\\bot',\n",
       " 239: '\\\\it',\n",
       " 240: '\\\\leftrightarrow',\n",
       " 241: '\\\\Rightarrow',\n",
       " 242: '\\\\parallel',\n",
       " 243: '\\\\mapsto',\n",
       " 244: '\\\\subset',\n",
       " 245: '\\\\textrm',\n",
       " 246: '\\\\iota',\n",
       " 247: '\\\\l',\n",
       " 248: '\\\\scriptsize',\n",
       " 249: '\\\\Bigg',\n",
       " 250: '\\\\left<',\n",
       " 251: '\\\\binom',\n",
       " 252: '\\\\overrightarrow',\n",
       " 253: '\\\\ll',\n",
       " 254: '\\\\jmath',\n",
       " 255: '\\\\phantom',\n",
       " 256: '\\\\sf',\n",
       " 257: '\\\\cong',\n",
       " 258: '\\\\tiny',\n",
       " 259: '\\\\ne',\n",
       " 260: '\\\\gg',\n",
       " 261: '\\\\Biggr',\n",
       " 262: '\\\\d',\n",
       " 263: '\\\\Biggl',\n",
       " 264: '\\\\o',\n",
       " 265: '\\\\Upsilon',\n",
       " 266: '\\\\breve',\n",
       " 267: '\\\\L',\n",
       " 268: '\\\\vee',\n",
       " 269: '\\\\bigoplus',\n",
       " 270: '\\\\bullet',\n",
       " 271: '\\\\small',\n",
       " 272: '\\\\wp',\n",
       " 273: '\\\\scriptstyle',\n",
       " 274: '\\\\atop',\n",
       " 275: '\\\\varpi',\n",
       " 276: '\\\\downarrow',\n",
       " 277: '\\\\kern',\n",
       " 278: '\\\\#',\n",
       " 279: '\\\\vdots',\n",
       " 280: '\\\\uparrow',\n",
       " 281: '\\\\cap',\n",
       " 282: '\\\\rbrack',\n",
       " 283: '\\\\Im',\n",
       " 284: '\\\\supset',\n",
       " 285: '\\\\sb',\n",
       " 286: '\\\\slash',\n",
       " 287: '\\\\hline',\n",
       " 288: '\\\\cup',\n",
       " 289: '\\\\Re',\n",
       " 290: '\\\\Longrightarrow',\n",
       " 291: '\\\\mit',\n",
       " 292: '\\\\upsilon',\n",
       " 293: '\\\\underbrace',\n",
       " 294: '\\\\acute',\n",
       " 295: '\\\\varsigma',\n",
       " 296: '\\\\lbrace',\n",
       " 297: '\\\\protect',\n",
       " 298: '\\\\rbrace',\n",
       " 299: '\\\\O',\n",
       " 300: '\\\\vspace',\n",
       " 301: '\\\\bigtriangleup',\n",
       " 302: '\\\\Leftrightarrow',\n",
       " 303: '\\\\S',\n",
       " 304: '\\\\mathsf',\n",
       " 305: '`',\n",
       " 306: '\\\\longleftrightarrow',\n",
       " 307: '\\\\i',\n",
       " 308: '\\\\leftarrow',\n",
       " 309: '\\\\Vert',\n",
       " 310: '\\\\footnotesize',\n",
       " 311: '\\\\ddots',\n",
       " 312: '\\\\rightharpoonup',\n",
       " 313: '\\\\Large',\n",
       " 314: '\\\\Longleftrightarrow',\n",
       " 315: '\\\\enspace',\n",
       " 316: '\\\\right\\\\vert',\n",
       " 317: 'mm',\n",
       " 318: '\\\\left\\\\vert',\n",
       " 319: '\\\\raisebox',\n",
       " 320: '\\\\cdotp',\n",
       " 321: 'ule',\n",
       " 322: '\\\\bigotimes',\n",
       " 323: '\\\\put',\n",
       " 324: '\\\\makebox',\n",
       " 325: '\\\\tt',\n",
       " 326: '\\\\emptyset',\n",
       " 327: '\\\\doteq',\n",
       " 328: '\\\\hfill',\n",
       " 329: '\\\\P',\n",
       " 330: '\\\\overleftarrow',\n",
       " 331: '\\\\large',\n",
       " 332: '\\\\left\\\\|',\n",
       " 333: '\\\\right\\\\|',\n",
       " 334: '\\\\textbf',\n",
       " 335: '\\\\mathop',\n",
       " 336: '\\\\vphantom',\n",
       " 337: '\\\\llap',\n",
       " 338: '\\\\backslash',\n",
       " 339: '\"',\n",
       " 340: '\\\\sharp',\n",
       " 341: '\\\\buildrel',\n",
       " 342: '\\\\raise',\n",
       " 343: '\\\\sl',\n",
       " 344: '\\\\flat',\n",
       " 345: '\\\\ref',\n",
       " 346: '\\\\odot',\n",
       " 347: '\\\\noalign',\n",
       " 348: '\\\\mathit',\n",
       " 349: '\\\\label',\n",
       " 350: '\\\\textup',\n",
       " 351: 'cm',\n",
       " 352: '\\\\bigcup',\n",
       " 353: '\\\\strut',\n",
       " 354: '\\\\/',\n",
       " 355: '\\\\longmapsto',\n",
       " 356: '\\\\rfloor',\n",
       " 357: '\\\\unitlength',\n",
       " 358: '\\\\overbrace',\n",
       " 359: '\\\\thinspace',\n",
       " 360: '\\\\colon',\n",
       " 361: '\\\\subseteq',\n",
       " 362: '\\\\setlength',\n",
       " 363: '\\\\ni',\n",
       " 364: '\\\\pounds',\n",
       " 365: '\\\\diamond',\n",
       " 366: '\\\\_',\n",
       " 367: '\\\\fbox',\n",
       " 368: '\\\\ominus',\n",
       " 369: '\\\\line',\n",
       " 370: '\\\\enskip',\n",
       " 371: '[object',\n",
       " 372: 'Object]',\n",
       " 373: '\\\\bigwedge',\n",
       " 374: '\\\\aleph',\n",
       " 375: '\\\\circle',\n",
       " 376: '?',\n",
       " 377: '\\\\bigtriangledown',\n",
       " 378: '\\\\lfloor',\n",
       " 379: '\\\\bigcap',\n",
       " 380: '\\\\vrule',\n",
       " 381: '\\\\smallskip',\n",
       " 382: '\\\\b',\n",
       " 383: '\\\\land',\n",
       " 384: '\\\\bmod',\n",
       " 385: '\\\\space',\n",
       " 386: '\\\\left\\\\lbrack',\n",
       " 387: '\\\\right\\\\rbrack',\n",
       " 388: '\\\\vskip',\n",
       " 389: '\\\\hookrightarrow',\n",
       " 390: '\\\\rlap',\n",
       " 391: '\\\\diamondsuit',\n",
       " 392: '\\\\hrule',\n",
       " 393: '\\\\natural'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.id2sign"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1024])\n"
     ]
    }
   ],
   "source": [
    "class model(nn.Module):\n",
    "    def __init__(self, hidden_size=, vocab_size):\n",
    "        \"\"\"args:\n",
    "        imgs: [B, C, H, W]\n",
    "        formulas: [B, MAX_LEN]\n",
    "        \"\"\"\n",
    "        self.hidden_size=hidden_size\n",
    "        self.vocab_size=vocab_size\n",
    "        self.embed_size=embed_size\n",
    "        \n",
    "        super(model, self).__init__()\n",
    "        self.cnn_encoder = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 1),\n",
    "\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 1),\n",
    "\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2, 1), (2, 1), 0),\n",
    "            \n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "    \n",
    "        # RNN Decoder\n",
    "        self.linear = nn.Linear(self.hidden_size, self.vocab_size)\n",
    "        self.embed = nn.Embedding(self.vocab_size, self.embed_size)\n",
    "        self.lstm_cell = nn.LSTMCell(input_size=self.embed_size, hidden_size=self.hidden_size)\n",
    "    \n",
    "    \n",
    "    def forward(self, images, tokenized_formulas):\n",
    "        # encoder part - just pass through and flatten\n",
    "        encoded_imgs = self.cnn_encoder(images)  # [B, 128, H', W']\n",
    "        features=torch.flatten(encoded_imgs,1) # [B, 128 * H' * W']\n",
    "        \n",
    "        # decoder part\n",
    "        batch_size = features.size(0) # 64 * H' * W'\n",
    "        \n",
    "        # init the hidden and cell states to zeros\n",
    "        hidden_state = torch.zeros((batch_size, self.hidden_size)).cuda()\n",
    "        cell_state = torch.zeros((batch_size, self.hidden_size)).cuda()\n",
    "    \n",
    "        # define the output tensor placeholder\n",
    "        outputs = torch.empty((batch_size, tokenized_formulas.size(1), self.vocab_size)).cuda()\n",
    "\n",
    "        # embed the formulas\n",
    "        formula_embed = self.embed(tokenized_formulas)\n",
    "        \n",
    "        # pass the caption word by word\n",
    "        for t in range(tokenized_formulas.size(1)):\n",
    "\n",
    "            # for the first time step the input is the feature vector\n",
    "            if t == 0:\n",
    "                hidden_state, cell_state = self.lstm_cell(features, (hidden_state, cell_state))\n",
    "                \n",
    "            # for the 2nd+ time step, using teacher forcer\n",
    "            else:\n",
    "                hidden_state, cell_state = self.lstm_cell(formula_embed[:, t, :], (hidden_state, cell_state))\n",
    "            \n",
    "            # output of the attention mechanism\n",
    "            out = self.fc_out(hidden_state)\n",
    "            \n",
    "            # build the output tensor\n",
    "            outputs[:, t, :] = out\n",
    "    \n",
    "        return outputs\n",
    "\n",
    "    \n",
    "enc = model(16)\n",
    "x = torch.randn((1, 1, 400, 400)) #imgs: [B, C, H, W]\n",
    "print(enc(x).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, hidden_size=64, embed_size, out_dimension):\n",
    "        super(Encoder, self).__init__()\n",
    "#         CNN Encoder\n",
    "        self.cnn_encoder = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=hidden_size, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 1),\n",
    "\n",
    "            n.Conv2d(in_channels=hidden_size, out_channels=hidden_size*2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 1),\n",
    "\n",
    "            n.Conv2d(in_channels=hidden_size*2, out_channels=hidden_size*4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            n.Conv2d(in_channels=hidden_size*4, out_channels=hidden_size*2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2, 1), (2, 1), 0),\n",
    "            \n",
    "            n.Conv2d(in_channels=hidden_size*2, out_channels=hidden_size, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        \n",
    "\n",
    "    def forward(self, images, formula):\n",
    "        features = self.cnn_encoder(images)\n",
    "        return self.dropout(self.relu(features))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
